{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* NAME: RISHABH PATIL\n",
        "* SAP: 60009200056\n",
        "* BATCH: K2"
      ],
      "metadata": {
        "id": "Y9tw-N6SpwfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# This is a straightforwad implementation of SARSA for the FrozenLake OpenAI\n",
        "# Gym testbed. I wrote it mostly to make myself familiar with the OpenAI gym;\n",
        "# the SARSA algorithm was implemented pretty much from the Wikipedia page alone.\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\")\n",
        "\n",
        "def choose_action(observation):\n",
        "    return np.argmax(q_table[observation])\n",
        "\n",
        "alpha = 0.4\n",
        "gamma = 0.999\n",
        "\n",
        "q_table = dict([(x, [1, 1, 1, 1]) for x in range(16)])\n",
        "score = []\n",
        "\n",
        "for i in range(5000):\n",
        "    observation = env.reset()\n",
        "    action = choose_action(observation)\n",
        "\n",
        "    prev_observation = None\n",
        "    prev_action      = None\n",
        "\n",
        "    t = 0\n",
        "\n",
        "    for t in range(500):\n",
        "        observation, reward, done, info = env.step(action)\n",
        "\n",
        "        action = choose_action(observation)\n",
        "\n",
        "        if not prev_observation is None:\n",
        "            q_old = q_table[prev_observation][prev_action]\n",
        "            q_new = q_old\n",
        "            if done:\n",
        "                q_new += alpha * (reward - q_old)\n",
        "            else:\n",
        "                q_new += alpha * (reward + gamma * q_table[observation][action] - q_old)\n",
        "\n",
        "            new_table = q_table[prev_observation]\n",
        "            new_table[prev_action] = q_new\n",
        "            q_table[prev_observation] = new_table\n",
        "\n",
        "        prev_observation = observation\n",
        "        prev_action = action\n",
        "\n",
        "        if done:\n",
        "            if len(score) < 100:\n",
        "                score.append(reward)\n",
        "            else:\n",
        "                score[i % 100] = reward\n",
        "\n",
        "            print(\"Episode {} finished after {} timesteps with r={}. Running score: {}\".format(i, t, reward, np.mean(score)))\n",
        "            break"
      ],
      "metadata": {
        "id": "9Q_HIwRa-LU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_table[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2OnRV0F-vYh",
        "outputId": "2293f1cd-f80d-4cc5-c110-fa13bf31c083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [0.7226263981689833,\n",
              "  0.289004567782087,\n",
              "  0.2670748779786122,\n",
              "  0.31060891589851186],\n",
              " 1: [0.216, 0.23669617630310513, 0.19372117048981807, 0.7765793826100182],\n",
              " 2: [0.21040046216560965,\n",
              "  0.21803945255755453,\n",
              "  0.22893151804021902,\n",
              "  0.6695472096775952],\n",
              " 3: [0.1663862020533836,\n",
              "  0.14572785686400305,\n",
              "  0.12917591170173842,\n",
              "  0.5927662205354964],\n",
              " 4: [0.8012575321107746,\n",
              "  0.24583884168939907,\n",
              "  0.19487121804013147,\n",
              "  0.21571683268447214],\n",
              " 5: [1, 1, 1, 1],\n",
              " 6: [0.006136624494970402,\n",
              "  0.005723614168731134,\n",
              "  0.34906501868694384,\n",
              "  0.009301147696468243],\n",
              " 7: [1, 1, 1, 1],\n",
              " 8: [0.26189142818632016,\n",
              "  0.21556834546178769,\n",
              "  0.25328811025299236,\n",
              "  0.8609342131387228],\n",
              " 9: [0.24713421903606073,\n",
              "  0.8916699039712425,\n",
              "  0.19529552705255038,\n",
              "  0.21569175785316033],\n",
              " 10: [0.9335305869002015,\n",
              "  0.0709147335528628,\n",
              "  0.08170639258202461,\n",
              "  0.09426550841888155],\n",
              " 11: [1, 1, 1, 1],\n",
              " 12: [1, 1, 1, 1],\n",
              " 13: [0.21586175999999993,\n",
              "  0.266240742969192,\n",
              "  0.9244459212714012,\n",
              "  0.2950954438397484],\n",
              " 14: [0.38170961321012065,\n",
              "  0.9924361710767282,\n",
              "  0.49843150147237847,\n",
              "  0.48465814631038306],\n",
              " 15: [1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}